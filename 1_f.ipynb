{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torchvision.transforms import Lambda, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[17, 14, 26, 25, 15, 18, 15,  9]]), tensor([[ 0, 26, 26,  6, 19, 19,  5, 11, 17]])]\n"
     ]
    }
   ],
   "source": [
    "# Create a Custom Dataset from train_data.csv and eval_data.csv\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pandas.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        # Remove the first row from the Dataframe\n",
    "        self.data = self.data.iloc[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() # Convert the tensor to a list\n",
    "        # Get the data from the Dataframe\n",
    "        data = self.data.iloc[idx]\n",
    "        # Convert the data to a numpy array\n",
    "        data = data[0] # First column of the Dataframe\n",
    "        data_num = []\n",
    "        for i in data:\n",
    "            data_num.append(ord(i)- 96)\n",
    "        data = np.array(data_num)\n",
    "        # Convert the data to a tensor\n",
    "        data = torch.from_numpy(data)\n",
    "        label = self.data.iloc[idx]\n",
    "        label = label[1]\n",
    "        label_num = []\n",
    "        for i in label:\n",
    "            label_num.append(ord(i)- 96)\n",
    "        # add 0 at pos 0 to label\n",
    "        label_num.insert(0, 0)\n",
    "        label = np.array(label_num)\n",
    "        if self.transform:\n",
    "            data = self.transform(data) # Apply the transform on the data\n",
    "        return data, label\n",
    "\n",
    "# Write a DataLoader for the Custom Dataset\n",
    "train_dataset = CustomDataset('./A3 files/train_data.csv', transform=None)\n",
    "\n",
    "# Split the Dataset into Train and Validation Datasets\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2)\n",
    "\n",
    "# Create a DataLoader for the Train and Validation Datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "############################# TESTING CODE #############################\n",
    "\n",
    "# Print out some data from the Train Dataset\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([14, 17, 16, 23, 18, 24, 12,  3]), array([ 0,  6, 14,  7,  4,  6, 17, 22, 12]))\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Injecting Some Information about the Relative or the Absolute Positioning of the tokens in the sequence\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len ):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        position = []\n",
    "        pos_enc = torch.zeros(max_len, d_model) # Positional Encoding -> Max Length and the dimensions of the model\n",
    "        for i in range(max_len):\n",
    "            position.append(i)\n",
    "        position = torch.tensor(position).unsqueeze(1) # Got the pos -> value\n",
    "        # Now, you want to make the position term to be max_len, d_model\n",
    "        position_stacked = [position] * d_model \n",
    "        position = torch.cat(position_stacked, dim=1)\n",
    "        # Now to obtain the 10000^2i/d_model \n",
    "        div_term = torch.arange(0, d_model, 2) # The Value to be divided\n",
    "        div_term = div_term/d_model\n",
    "        div_term = div_term.type(torch.float64)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))   \n",
    "        pos_enc[:, 0::2] = torch.sin(position[:, 0::2]*div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position[:, 1::2]*div_term)\n",
    "        self.pos_enc = pos_enc\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc # This is placeholder -> Modify to include only the len given -> Not Max Len\n",
    "        #print(\"Positional Encoding Completed\")\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Create a Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len = 8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len = 9)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        self.linear = nn.Linear(d_model, 27)\n",
    "        return\n",
    "    \n",
    "    def forward(self, target, memory):\n",
    "        target = self.pos_enc(target)\n",
    "        target_mask = torch.triu(torch.ones(9, 9), diagonal=1)\n",
    "        # reshape target and memory\n",
    "        target = target.permute(1, 0, 2)\n",
    "        memory = memory.permute(1, 0, 2)\n",
    "        #print(target.shape, memory.shape, target_mask.shape, \"------>\")\n",
    "        x = self.transformer_decoder(target, memory)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(27, d_model)\n",
    "        self.embed_outputs = nn.Embedding(27, d_model)\n",
    "        self.encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        self.decoder = TransformerDecoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = self.embedding(x) # X = batch_size, seq_len, d_model\n",
    "        x = self.encoder(x) \n",
    "        y = self.embed_outputs(y)\n",
    "        y = self.decoder(y, x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch:  1 Batch:  1 Accuracy:  0.25 Loss:  24.400714874267578\n",
      "Epoch:  1 Batch:  2 Accuracy:  0.0 Loss:  29.97183609008789\n",
      "Epoch:  1 Batch:  3 Accuracy:  0.0 Loss:  34.97869873046875\n",
      "Epoch:  1 Batch:  4 Accuracy:  0.0 Loss:  26.695940017700195\n",
      "Epoch:  1 Batch:  5 Accuracy:  0.0 Loss:  28.882400512695312\n",
      "Epoch:  1 Batch:  6 Accuracy:  0.0 Loss:  30.11745834350586\n",
      "Epoch:  1 Batch:  7 Accuracy:  0.125 Loss:  28.767696380615234\n",
      "Epoch:  1 Batch:  8 Accuracy:  0.0 Loss:  29.706167221069336\n",
      "Epoch:  1 Batch:  9 Accuracy:  0.125 Loss:  31.31991958618164\n",
      "Epoch:  1 Batch:  10 Accuracy:  0.0 Loss:  28.47806739807129\n",
      "Epoch:  1 Batch:  11 Accuracy:  0.125 Loss:  26.855194091796875\n",
      "Epoch:  1 Batch:  12 Accuracy:  0.0 Loss:  29.770885467529297\n",
      "Epoch:  1 Batch:  13 Accuracy:  0.0 Loss:  29.270078659057617\n",
      "Epoch:  1 Batch:  14 Accuracy:  0.0 Loss:  26.626773834228516\n",
      "Epoch:  1 Batch:  15 Accuracy:  0.375 Loss:  25.200061798095703\n",
      "Epoch:  1 Batch:  16 Accuracy:  0.0 Loss:  26.98584747314453\n",
      "Epoch:  1 Batch:  17 Accuracy:  0.125 Loss:  32.595638275146484\n",
      "Epoch:  1 Batch:  18 Accuracy:  0.0 Loss:  27.523883819580078\n",
      "Epoch:  1 Batch:  19 Accuracy:  0.25 Loss:  24.604585647583008\n",
      "Epoch:  1 Batch:  20 Accuracy:  0.125 Loss:  25.729564666748047\n",
      "Epoch:  1 Batch:  21 Accuracy:  0.0 Loss:  30.463857650756836\n",
      "Epoch:  1 Batch:  22 Accuracy:  0.125 Loss:  24.583690643310547\n",
      "Epoch:  1 Batch:  23 Accuracy:  0.125 Loss:  27.951372146606445\n",
      "Epoch:  1 Batch:  24 Accuracy:  0.125 Loss:  26.462921142578125\n",
      "Epoch:  1 Batch:  25 Accuracy:  0.125 Loss:  28.60674285888672\n",
      "Epoch:  1 Batch:  26 Accuracy:  0.125 Loss:  26.424612045288086\n",
      "Epoch:  1 Batch:  27 Accuracy:  0.0 Loss:  25.290054321289062\n",
      "Epoch:  1 Batch:  28 Accuracy:  0.0 Loss:  28.136587142944336\n",
      "Epoch:  1 Batch:  29 Accuracy:  0.125 Loss:  26.634178161621094\n",
      "Epoch:  1 Batch:  30 Accuracy:  0.0 Loss:  26.919784545898438\n",
      "Epoch:  1 Batch:  31 Accuracy:  0.0 Loss:  28.634563446044922\n",
      "Epoch:  1 Batch:  32 Accuracy:  0.0 Loss:  29.790233612060547\n",
      "Epoch:  1 Batch:  33 Accuracy:  0.25 Loss:  24.39853286743164\n",
      "Epoch:  1 Batch:  34 Accuracy:  0.25 Loss:  24.543581008911133\n",
      "Epoch:  1 Batch:  35 Accuracy:  0.0 Loss:  29.596960067749023\n",
      "Epoch:  1 Batch:  36 Accuracy:  0.0 Loss:  29.86822509765625\n",
      "Epoch:  1 Batch:  37 Accuracy:  0.0 Loss:  28.672847747802734\n",
      "Epoch:  1 Batch:  38 Accuracy:  0.0 Loss:  26.31955337524414\n",
      "Epoch:  1 Batch:  39 Accuracy:  0.0 Loss:  28.737361907958984\n",
      "Epoch:  1 Batch:  40 Accuracy:  0.0 Loss:  29.536636352539062\n",
      "Epoch:  1 Batch:  41 Accuracy:  0.125 Loss:  25.650466918945312\n",
      "Epoch:  1 Batch:  42 Accuracy:  0.125 Loss:  26.087955474853516\n",
      "Epoch:  1 Batch:  43 Accuracy:  0.125 Loss:  27.39105987548828\n",
      "Epoch:  1 Batch:  44 Accuracy:  0.125 Loss:  23.538684844970703\n",
      "Epoch:  1 Batch:  45 Accuracy:  0.0 Loss:  26.130218505859375\n",
      "Epoch:  1 Batch:  46 Accuracy:  0.0 Loss:  27.762269973754883\n",
      "Epoch:  1 Batch:  47 Accuracy:  0.125 Loss:  27.964906692504883\n",
      "Epoch:  1 Batch:  48 Accuracy:  0.125 Loss:  25.803773880004883\n",
      "Epoch:  1 Batch:  49 Accuracy:  0.0 Loss:  27.174795150756836\n",
      "Epoch:  1 Batch:  50 Accuracy:  0.125 Loss:  25.29344940185547\n",
      "Epoch:  1 Batch:  51 Accuracy:  0.0 Loss:  27.567523956298828\n",
      "Epoch:  1 Batch:  52 Accuracy:  0.0 Loss:  26.33397674560547\n",
      "Epoch:  1 Batch:  53 Accuracy:  0.125 Loss:  24.953960418701172\n",
      "Epoch:  1 Batch:  54 Accuracy:  0.25 Loss:  23.44021224975586\n",
      "Epoch:  1 Batch:  55 Accuracy:  0.0 Loss:  28.37845230102539\n",
      "Epoch:  1 Batch:  56 Accuracy:  0.0 Loss:  26.7113037109375\n",
      "Epoch:  1 Batch:  57 Accuracy:  0.0 Loss:  28.132558822631836\n",
      "Epoch:  1 Batch:  58 Accuracy:  0.0 Loss:  28.57004165649414\n",
      "Epoch:  1 Batch:  59 Accuracy:  0.125 Loss:  25.167457580566406\n",
      "Epoch:  1 Batch:  60 Accuracy:  0.0 Loss:  29.734195709228516\n",
      "Epoch:  1 Batch:  61 Accuracy:  0.125 Loss:  26.86348533630371\n",
      "Epoch:  1 Batch:  62 Accuracy:  0.375 Loss:  25.20164680480957\n",
      "Epoch:  1 Batch:  63 Accuracy:  0.0 Loss:  25.87483024597168\n",
      "Epoch:  1 Batch:  64 Accuracy:  0.0 Loss:  26.84423828125\n",
      "Epoch:  1 Batch:  65 Accuracy:  0.125 Loss:  25.886531829833984\n",
      "Epoch:  1 Batch:  66 Accuracy:  0.0 Loss:  26.698631286621094\n",
      "Epoch:  1 Batch:  67 Accuracy:  0.0 Loss:  28.677528381347656\n",
      "Epoch:  1 Batch:  68 Accuracy:  0.25 Loss:  28.394826889038086\n",
      "Epoch:  1 Batch:  69 Accuracy:  0.125 Loss:  27.072418212890625\n",
      "Epoch:  1 Batch:  70 Accuracy:  0.125 Loss:  26.167373657226562\n",
      "Epoch:  1 Batch:  71 Accuracy:  0.0 Loss:  28.408924102783203\n",
      "Epoch:  1 Batch:  72 Accuracy:  0.125 Loss:  27.380525588989258\n",
      "Epoch:  1 Batch:  73 Accuracy:  0.0 Loss:  26.50255584716797\n",
      "Epoch:  1 Batch:  74 Accuracy:  0.0 Loss:  27.72538185119629\n",
      "Epoch:  1 Batch:  75 Accuracy:  0.0 Loss:  28.41229820251465\n",
      "Epoch:  1 Batch:  76 Accuracy:  0.0 Loss:  26.98826789855957\n",
      "Epoch:  1 Batch:  77 Accuracy:  0.0 Loss:  26.435840606689453\n",
      "Epoch:  1 Batch:  78 Accuracy:  0.0 Loss:  26.104013442993164\n",
      "Epoch:  1 Batch:  79 Accuracy:  0.0 Loss:  26.343673706054688\n",
      "Epoch:  1 Batch:  80 Accuracy:  0.375 Loss:  23.402502059936523\n",
      "Epoch:  1 Batch:  81 Accuracy:  0.125 Loss:  27.746490478515625\n",
      "Epoch:  1 Batch:  82 Accuracy:  0.0 Loss:  30.189767837524414\n",
      "Epoch:  1 Batch:  83 Accuracy:  0.0 Loss:  27.635425567626953\n",
      "Epoch:  1 Batch:  84 Accuracy:  0.25 Loss:  24.540931701660156\n",
      "Epoch:  1 Batch:  85 Accuracy:  0.25 Loss:  24.125141143798828\n",
      "Epoch:  1 Batch:  86 Accuracy:  0.0 Loss:  28.596389770507812\n",
      "Epoch:  1 Batch:  87 Accuracy:  0.0 Loss:  27.48334503173828\n",
      "Epoch:  1 Batch:  88 Accuracy:  0.125 Loss:  28.30402946472168\n",
      "Epoch:  1 Batch:  89 Accuracy:  0.125 Loss:  27.389331817626953\n",
      "Epoch:  1 Batch:  90 Accuracy:  0.375 Loss:  22.46847915649414\n",
      "Epoch:  1 Batch:  91 Accuracy:  0.25 Loss:  23.203102111816406\n",
      "Epoch:  1 Batch:  92 Accuracy:  0.0 Loss:  27.369617462158203\n",
      "Epoch:  1 Batch:  93 Accuracy:  0.0 Loss:  30.781450271606445\n",
      "Epoch:  1 Batch:  94 Accuracy:  0.0 Loss:  27.899383544921875\n",
      "Epoch:  1 Batch:  95 Accuracy:  0.125 Loss:  25.13240623474121\n",
      "Epoch:  1 Batch:  96 Accuracy:  0.0 Loss:  27.556238174438477\n",
      "Epoch:  1 Batch:  97 Accuracy:  0.125 Loss:  28.042102813720703\n",
      "Epoch:  1 Batch:  98 Accuracy:  0.125 Loss:  27.259435653686523\n",
      "Epoch:  1 Batch:  99 Accuracy:  0.0 Loss:  26.836627960205078\n",
      "Epoch:  1 Batch:  100 Accuracy:  0.25 Loss:  22.575807571411133\n",
      "Epoch:  1 Batch:  101 Accuracy:  0.0 Loss:  27.021574020385742\n",
      "Epoch:  1 Batch:  102 Accuracy:  0.125 Loss:  27.302982330322266\n",
      "Epoch:  1 Batch:  103 Accuracy:  0.0 Loss:  28.614059448242188\n",
      "Epoch:  1 Batch:  104 Accuracy:  0.125 Loss:  29.398590087890625\n",
      "Epoch:  1 Batch:  105 Accuracy:  0.0 Loss:  26.531435012817383\n",
      "Epoch:  1 Batch:  106 Accuracy:  0.0 Loss:  26.58283042907715\n",
      "Epoch:  1 Batch:  107 Accuracy:  0.0 Loss:  29.073827743530273\n",
      "Epoch:  1 Batch:  108 Accuracy:  0.125 Loss:  26.079959869384766\n",
      "Epoch:  1 Batch:  109 Accuracy:  0.0 Loss:  28.06376075744629\n",
      "Epoch:  1 Batch:  110 Accuracy:  0.0 Loss:  26.592266082763672\n",
      "Epoch:  1 Batch:  111 Accuracy:  0.125 Loss:  24.95594024658203\n",
      "Epoch:  1 Batch:  112 Accuracy:  0.25 Loss:  25.47708511352539\n",
      "Epoch:  1 Batch:  113 Accuracy:  0.0 Loss:  27.2637996673584\n",
      "Epoch:  1 Batch:  114 Accuracy:  0.0 Loss:  23.458473205566406\n",
      "Epoch:  1 Batch:  115 Accuracy:  0.125 Loss:  26.695905685424805\n",
      "Epoch:  1 Batch:  116 Accuracy:  0.125 Loss:  27.96407127380371\n",
      "Epoch:  1 Batch:  117 Accuracy:  0.0 Loss:  28.53898811340332\n",
      "Epoch:  1 Batch:  118 Accuracy:  0.0 Loss:  26.499481201171875\n",
      "Epoch:  1 Batch:  119 Accuracy:  0.25 Loss:  25.9265193939209\n",
      "Epoch:  1 Batch:  120 Accuracy:  0.125 Loss:  27.462318420410156\n",
      "Epoch:  1 Batch:  121 Accuracy:  0.0 Loss:  27.919198989868164\n",
      "Epoch:  1 Batch:  122 Accuracy:  0.0 Loss:  27.405292510986328\n",
      "Epoch:  1 Batch:  123 Accuracy:  0.125 Loss:  28.559968948364258\n",
      "Epoch:  1 Batch:  124 Accuracy:  0.0 Loss:  27.214977264404297\n",
      "Epoch:  1 Batch:  125 Accuracy:  0.0 Loss:  27.206501007080078\n",
      "Epoch:  1 Batch:  126 Accuracy:  0.0 Loss:  27.681669235229492\n",
      "Epoch:  1 Batch:  127 Accuracy:  0.0 Loss:  28.17066764831543\n",
      "Epoch:  1 Batch:  128 Accuracy:  0.0 Loss:  27.293251037597656\n",
      "Epoch:  1 Batch:  129 Accuracy:  0.0 Loss:  27.147876739501953\n",
      "Epoch:  1 Batch:  130 Accuracy:  0.125 Loss:  24.91720962524414\n",
      "Epoch:  1 Batch:  131 Accuracy:  0.0 Loss:  25.656034469604492\n",
      "Epoch:  1 Batch:  132 Accuracy:  0.125 Loss:  25.201269149780273\n",
      "Epoch:  1 Batch:  133 Accuracy:  0.25 Loss:  25.48760223388672\n",
      "Epoch:  1 Batch:  134 Accuracy:  0.125 Loss:  26.176389694213867\n",
      "Epoch:  1 Batch:  135 Accuracy:  0.0 Loss:  27.389720916748047\n",
      "Epoch:  1 Batch:  136 Accuracy:  0.125 Loss:  26.381336212158203\n",
      "Epoch:  1 Batch:  137 Accuracy:  0.125 Loss:  25.77016258239746\n",
      "Epoch:  1 Batch:  138 Accuracy:  0.0 Loss:  28.218061447143555\n",
      "Epoch:  1 Batch:  139 Accuracy:  0.0 Loss:  27.79202651977539\n",
      "Epoch:  1 Batch:  140 Accuracy:  0.125 Loss:  26.001190185546875\n",
      "Epoch:  1 Batch:  141 Accuracy:  0.0 Loss:  26.02371597290039\n",
      "Epoch:  1 Batch:  142 Accuracy:  0.0 Loss:  26.64148712158203\n",
      "Epoch:  1 Batch:  143 Accuracy:  0.0 Loss:  26.761001586914062\n",
      "Epoch:  1 Batch:  144 Accuracy:  0.0 Loss:  28.175233840942383\n",
      "Epoch:  1 Batch:  145 Accuracy:  0.125 Loss:  24.261762619018555\n",
      "Epoch:  1 Batch:  146 Accuracy:  0.0 Loss:  28.781612396240234\n",
      "Epoch:  1 Batch:  147 Accuracy:  0.125 Loss:  26.715129852294922\n",
      "Epoch:  1 Batch:  148 Accuracy:  0.125 Loss:  29.71731948852539\n",
      "Epoch:  1 Batch:  149 Accuracy:  0.0 Loss:  26.596513748168945\n",
      "Epoch:  1 Batch:  150 Accuracy:  0.0 Loss:  29.542139053344727\n",
      "Epoch:  1 Batch:  151 Accuracy:  0.0 Loss:  26.998085021972656\n",
      "Epoch:  1 Batch:  152 Accuracy:  0.125 Loss:  27.96327781677246\n",
      "Epoch:  1 Batch:  153 Accuracy:  0.125 Loss:  26.142501831054688\n",
      "Epoch:  1 Batch:  154 Accuracy:  0.125 Loss:  26.261051177978516\n",
      "Epoch:  1 Batch:  155 Accuracy:  0.25 Loss:  26.194021224975586\n",
      "Epoch:  1 Batch:  156 Accuracy:  0.125 Loss:  25.54817008972168\n",
      "Epoch:  1 Batch:  157 Accuracy:  0.0 Loss:  28.372726440429688\n",
      "Epoch:  1 Batch:  158 Accuracy:  0.0 Loss:  27.770389556884766\n",
      "Epoch:  1 Batch:  159 Accuracy:  0.125 Loss:  25.442941665649414\n",
      "Epoch:  1 Batch:  160 Accuracy:  0.25 Loss:  25.162744522094727\n",
      "Epoch:  1 Batch:  161 Accuracy:  0.0 Loss:  26.91607093811035\n",
      "Epoch:  1 Batch:  162 Accuracy:  0.0 Loss:  27.093626022338867\n",
      "Epoch:  1 Batch:  163 Accuracy:  0.125 Loss:  26.313880920410156\n",
      "Epoch:  1 Batch:  164 Accuracy:  0.0 Loss:  27.651262283325195\n",
      "Epoch:  1 Batch:  165 Accuracy:  0.0 Loss:  28.39399528503418\n",
      "Epoch:  1 Batch:  166 Accuracy:  0.0 Loss:  27.325328826904297\n",
      "Epoch:  1 Batch:  167 Accuracy:  0.125 Loss:  25.772361755371094\n",
      "Epoch:  1 Batch:  168 Accuracy:  0.0 Loss:  26.592323303222656\n",
      "Epoch:  1 Batch:  169 Accuracy:  0.125 Loss:  26.715505599975586\n",
      "Epoch:  1 Batch:  170 Accuracy:  0.0 Loss:  25.172014236450195\n",
      "Epoch:  1 Batch:  171 Accuracy:  0.0 Loss:  27.327180862426758\n",
      "Epoch:  1 Batch:  172 Accuracy:  0.125 Loss:  24.618370056152344\n",
      "Epoch:  1 Batch:  173 Accuracy:  0.125 Loss:  24.920141220092773\n",
      "Epoch:  1 Batch:  174 Accuracy:  0.0 Loss:  28.141115188598633\n",
      "Epoch:  1 Batch:  175 Accuracy:  0.125 Loss:  27.51128387451172\n",
      "Epoch:  1 Batch:  176 Accuracy:  0.125 Loss:  27.91986083984375\n",
      "Epoch:  1 Batch:  177 Accuracy:  0.25 Loss:  25.253944396972656\n",
      "Epoch:  1 Batch:  178 Accuracy:  0.125 Loss:  25.68659019470215\n",
      "Epoch:  1 Batch:  179 Accuracy:  0.125 Loss:  28.365894317626953\n",
      "Epoch:  1 Batch:  180 Accuracy:  0.0 Loss:  25.54793357849121\n",
      "Epoch:  1 Batch:  181 Accuracy:  0.0 Loss:  26.645450592041016\n",
      "Epoch:  1 Batch:  182 Accuracy:  0.125 Loss:  25.044544219970703\n",
      "Epoch:  1 Batch:  183 Accuracy:  0.0 Loss:  26.45774269104004\n",
      "Epoch:  1 Batch:  184 Accuracy:  0.125 Loss:  25.304244995117188\n",
      "Epoch:  1 Batch:  185 Accuracy:  0.125 Loss:  25.943248748779297\n",
      "Epoch:  1 Batch:  186 Accuracy:  0.0 Loss:  26.0960693359375\n",
      "Epoch:  1 Batch:  187 Accuracy:  0.125 Loss:  25.191465377807617\n",
      "Epoch:  1 Batch:  188 Accuracy:  0.25 Loss:  24.228179931640625\n",
      "Epoch:  1 Batch:  189 Accuracy:  0.25 Loss:  26.17104721069336\n",
      "Epoch:  1 Batch:  190 Accuracy:  0.125 Loss:  25.1118106842041\n",
      "Epoch:  1 Batch:  191 Accuracy:  0.0 Loss:  26.945261001586914\n",
      "Epoch:  1 Batch:  192 Accuracy:  0.125 Loss:  26.405487060546875\n",
      "Epoch:  1 Batch:  193 Accuracy:  0.0 Loss:  27.05542755126953\n",
      "Epoch:  1 Batch:  194 Accuracy:  0.0 Loss:  26.972606658935547\n",
      "Epoch:  1 Batch:  195 Accuracy:  0.125 Loss:  26.820552825927734\n",
      "Epoch:  1 Batch:  196 Accuracy:  0.0 Loss:  28.813358306884766\n",
      "Epoch:  1 Batch:  197 Accuracy:  0.125 Loss:  26.62415885925293\n",
      "Epoch:  1 Batch:  198 Accuracy:  0.0 Loss:  27.894996643066406\n",
      "Epoch:  1 Batch:  199 Accuracy:  0.125 Loss:  26.551448822021484\n",
      "Epoch:  1 Batch:  200 Accuracy:  0.375 Loss:  24.525678634643555\n",
      "Epoch:  1 Batch:  201 Accuracy:  0.125 Loss:  25.032100677490234\n",
      "Epoch:  1 Batch:  202 Accuracy:  0.0 Loss:  25.89950942993164\n",
      "Epoch:  1 Batch:  203 Accuracy:  0.0 Loss:  26.455341339111328\n",
      "Epoch:  1 Batch:  204 Accuracy:  0.125 Loss:  25.521331787109375\n",
      "Epoch:  1 Batch:  205 Accuracy:  0.0 Loss:  26.65066146850586\n",
      "Epoch:  1 Batch:  206 Accuracy:  0.0 Loss:  24.925857543945312\n",
      "Epoch:  1 Batch:  207 Accuracy:  0.0 Loss:  27.19886589050293\n",
      "Epoch:  1 Batch:  208 Accuracy:  0.125 Loss:  24.661937713623047\n",
      "Epoch:  1 Batch:  209 Accuracy:  0.0 Loss:  27.64409065246582\n",
      "Epoch:  1 Batch:  210 Accuracy:  0.0 Loss:  27.22151756286621\n",
      "Epoch:  1 Batch:  211 Accuracy:  0.0 Loss:  27.972190856933594\n",
      "Epoch:  1 Batch:  212 Accuracy:  0.0 Loss:  26.75424575805664\n",
      "Epoch:  1 Batch:  213 Accuracy:  0.0 Loss:  25.70282745361328\n",
      "Epoch:  1 Batch:  214 Accuracy:  0.25 Loss:  23.670602798461914\n",
      "Epoch:  1 Batch:  215 Accuracy:  0.0 Loss:  26.68824005126953\n",
      "Epoch:  1 Batch:  216 Accuracy:  0.0 Loss:  28.665821075439453\n",
      "Epoch:  1 Batch:  217 Accuracy:  0.0 Loss:  28.15859603881836\n",
      "Epoch:  1 Batch:  218 Accuracy:  0.125 Loss:  25.983524322509766\n",
      "Epoch:  1 Batch:  219 Accuracy:  0.125 Loss:  24.938798904418945\n",
      "Epoch:  1 Batch:  220 Accuracy:  0.0 Loss:  27.88803482055664\n",
      "Epoch:  1 Batch:  221 Accuracy:  0.0 Loss:  25.86621856689453\n",
      "Epoch:  1 Batch:  222 Accuracy:  0.0 Loss:  25.450237274169922\n",
      "Epoch:  1 Batch:  223 Accuracy:  0.0 Loss:  27.70118522644043\n",
      "Epoch:  1 Batch:  224 Accuracy:  0.125 Loss:  25.687707901000977\n",
      "Epoch:  1 Batch:  225 Accuracy:  0.125 Loss:  25.449016571044922\n",
      "Epoch:  1 Batch:  226 Accuracy:  0.25 Loss:  26.256315231323242\n",
      "Epoch:  1 Batch:  227 Accuracy:  0.0 Loss:  26.115266799926758\n",
      "Epoch:  1 Batch:  228 Accuracy:  0.0 Loss:  27.194278717041016\n",
      "Epoch:  1 Batch:  229 Accuracy:  0.125 Loss:  25.07923698425293\n",
      "Epoch:  1 Batch:  230 Accuracy:  0.125 Loss:  25.184070587158203\n",
      "Epoch:  1 Batch:  231 Accuracy:  0.0 Loss:  27.903072357177734\n",
      "Epoch:  1 Batch:  232 Accuracy:  0.0 Loss:  25.780561447143555\n",
      "Epoch:  1 Batch:  233 Accuracy:  0.0 Loss:  27.914968490600586\n",
      "Epoch:  1 Batch:  234 Accuracy:  0.0 Loss:  28.714134216308594\n",
      "Epoch:  1 Batch:  235 Accuracy:  0.0 Loss:  27.326045989990234\n",
      "Epoch:  1 Batch:  236 Accuracy:  0.0 Loss:  25.751123428344727\n",
      "Epoch:  1 Batch:  237 Accuracy:  0.0 Loss:  28.173866271972656\n",
      "Epoch:  1 Batch:  238 Accuracy:  0.125 Loss:  25.37118148803711\n",
      "Epoch:  1 Batch:  239 Accuracy:  0.0 Loss:  27.50695037841797\n",
      "Epoch:  1 Batch:  240 Accuracy:  0.125 Loss:  25.49523162841797\n",
      "Epoch:  1 Batch:  241 Accuracy:  0.0 Loss:  28.813396453857422\n",
      "Epoch:  1 Batch:  242 Accuracy:  0.0 Loss:  25.933425903320312\n",
      "Epoch:  1 Batch:  243 Accuracy:  0.125 Loss:  28.286006927490234\n",
      "Epoch:  1 Batch:  244 Accuracy:  0.125 Loss:  26.55483627319336\n",
      "Epoch:  1 Batch:  245 Accuracy:  0.0 Loss:  26.438886642456055\n",
      "Epoch:  1 Batch:  246 Accuracy:  0.0 Loss:  25.969730377197266\n",
      "Epoch:  1 Batch:  247 Accuracy:  0.125 Loss:  27.03726577758789\n",
      "Epoch:  1 Batch:  248 Accuracy:  0.0 Loss:  28.702980041503906\n",
      "Epoch:  1 Batch:  249 Accuracy:  0.0 Loss:  26.27798080444336\n",
      "Epoch:  1 Batch:  250 Accuracy:  0.0 Loss:  26.970890045166016\n",
      "Epoch:  1 Batch:  251 Accuracy:  0.125 Loss:  26.832244873046875\n",
      "Epoch:  1 Batch:  252 Accuracy:  0.0 Loss:  26.814388275146484\n",
      "Epoch:  1 Batch:  253 Accuracy:  0.125 Loss:  27.54241180419922\n",
      "Epoch:  1 Batch:  254 Accuracy:  0.0 Loss:  25.87074089050293\n",
      "Epoch:  1 Batch:  255 Accuracy:  0.0 Loss:  25.24587631225586\n",
      "Epoch:  1 Batch:  256 Accuracy:  0.0 Loss:  26.40104103088379\n",
      "Epoch:  1 Batch:  257 Accuracy:  0.125 Loss:  24.888927459716797\n",
      "Epoch:  1 Batch:  258 Accuracy:  0.0 Loss:  25.770965576171875\n",
      "Epoch:  1 Batch:  259 Accuracy:  0.125 Loss:  26.860774993896484\n",
      "Epoch:  1 Batch:  260 Accuracy:  0.0 Loss:  26.010404586791992\n",
      "Epoch:  1 Batch:  261 Accuracy:  0.25 Loss:  25.895544052124023\n",
      "Epoch:  1 Batch:  262 Accuracy:  0.125 Loss:  26.938251495361328\n",
      "Epoch:  1 Batch:  263 Accuracy:  0.125 Loss:  25.992774963378906\n",
      "Epoch:  1 Batch:  264 Accuracy:  0.25 Loss:  24.3219051361084\n",
      "Epoch:  1 Batch:  265 Accuracy:  0.125 Loss:  25.887815475463867\n",
      "Epoch:  1 Batch:  266 Accuracy:  0.0 Loss:  24.991920471191406\n",
      "Epoch:  1 Batch:  267 Accuracy:  0.0 Loss:  26.94491195678711\n",
      "Epoch:  1 Batch:  268 Accuracy:  0.0 Loss:  26.602859497070312\n",
      "Epoch:  1 Batch:  269 Accuracy:  0.375 Loss:  26.354026794433594\n",
      "Epoch:  1 Batch:  270 Accuracy:  0.0 Loss:  27.798398971557617\n",
      "Epoch:  1 Batch:  271 Accuracy:  0.0 Loss:  30.675718307495117\n",
      "Epoch:  1 Batch:  272 Accuracy:  0.125 Loss:  25.626646041870117\n",
      "Epoch:  1 Batch:  273 Accuracy:  0.125 Loss:  26.03226089477539\n",
      "Epoch:  1 Batch:  274 Accuracy:  0.25 Loss:  25.51205825805664\n",
      "Epoch:  1 Batch:  275 Accuracy:  0.0 Loss:  28.98678970336914\n",
      "Epoch:  1 Batch:  276 Accuracy:  0.0 Loss:  27.079587936401367\n",
      "Epoch:  1 Batch:  277 Accuracy:  0.25 Loss:  24.237293243408203\n",
      "Epoch:  1 Batch:  278 Accuracy:  0.375 Loss:  22.722515106201172\n",
      "Epoch:  1 Batch:  279 Accuracy:  0.0 Loss:  29.53826904296875\n",
      "Epoch:  1 Batch:  280 Accuracy:  0.0 Loss:  27.915109634399414\n",
      "Epoch:  1 Batch:  281 Accuracy:  0.0 Loss:  27.294780731201172\n",
      "Epoch:  1 Batch:  282 Accuracy:  0.125 Loss:  26.774147033691406\n",
      "Epoch:  1 Batch:  283 Accuracy:  0.0 Loss:  27.431800842285156\n",
      "Epoch:  1 Batch:  284 Accuracy:  0.125 Loss:  27.43375015258789\n",
      "Epoch:  1 Batch:  285 Accuracy:  0.125 Loss:  26.3753604888916\n",
      "Epoch:  1 Batch:  286 Accuracy:  0.0 Loss:  27.028640747070312\n",
      "Epoch:  1 Batch:  287 Accuracy:  0.0 Loss:  26.970035552978516\n",
      "Epoch:  1 Batch:  288 Accuracy:  0.0 Loss:  27.099163055419922\n",
      "Epoch:  1 Batch:  289 Accuracy:  0.0 Loss:  24.83303451538086\n",
      "Epoch:  1 Batch:  290 Accuracy:  0.0 Loss:  28.024036407470703\n",
      "Epoch:  1 Batch:  291 Accuracy:  0.125 Loss:  25.554790496826172\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m     62\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     train_loop(train_dataloader, Model, criterion, optimizer)\n",
      "Cell \u001b[0;32mIn[60], line 32\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     30\u001b[0m y_loc_tensor \u001b[39m=\u001b[39m y_loc_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39m#print(\"In shape: \", y_loc_tensor.shape)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m output \u001b[39m=\u001b[39m model(X, y_loc_tensor)\n\u001b[1;32m     33\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m     34\u001b[0m preds \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39margmax(\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 88\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n\u001b[1;32m     87\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x) \u001b[39m# X = batch_size, seq_len, d_model\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x) \n\u001b[1;32m     89\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_outputs(y)\n\u001b[1;32m     90\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(y, x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 47\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     46\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_enc(x)\n\u001b[0;32m---> 47\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_encoder(x)\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    384\u001b[0m is_causal \u001b[39m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 387\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    389\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    390\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m, src\u001b[39m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:708\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39mis_causal))\n\u001b[0;32m--> 708\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ff_block(x))\n\u001b[1;32m    710\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:723\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ff_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 723\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear1(x))))\n\u001b[1;32m    724\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a Transformer Model\n",
    "Model = Transformer(300, 2, 2, 300, 0.1, \"relu\")\n",
    "\n",
    "# Create a Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an Optimizer\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr=0.0001)\n",
    "\n",
    "# Create a Training Loop\n",
    "acc_tot = 0\n",
    "tot_cnt = 0\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    acc_tot = 0\n",
    "    tot_cnt = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, ( X, y) in enumerate(dataloader):\n",
    "            y_loc = []\n",
    "            # append \n",
    "            # append 9 0s to y_loc\n",
    "            for i in range(9):\n",
    "                y_loc.append(0)\n",
    "            #print(y_loc, \"---------------->\")\n",
    "            tot_loss = 0\n",
    "            for i in range(8):\n",
    "                # convert y_loc to tensor of shape 1*seq_le\n",
    "                #print(y_loc)\n",
    "                y_loc_tensor = torch.tensor(y_loc)\n",
    "                # make it 2 dim\n",
    "                y_loc_tensor = y_loc_tensor.unsqueeze(0)\n",
    "                #print(\"In shape: \", y_loc_tensor.shape)\n",
    "                output = model(X, y_loc_tensor)\n",
    "                output = output.permute(1,0,2)\n",
    "                preds = output.argmax(2)\n",
    "                out_here = output[:, i+1, :]\n",
    "                #print(\"Out here shape: \", out_here.shape)\n",
    "                target = y[:, i+1]\n",
    "                loss = loss_fn(out_here, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                tot_loss += loss\n",
    "            # back propogate total loss\n",
    "            # tot_loss.backward()\n",
    "            # # update parameters\n",
    "            # optimizer.step()\n",
    "            # # zero the gradients\n",
    "            # optimizer.zero_grad()\n",
    "            # calc acc based on preds and y\n",
    "            preds = preds.squeeze(0)\n",
    "            #print(preds.shape, y.shape)\n",
    "            accuracy = (preds == y).sum()\n",
    "            accuracy = accuracy.item()\n",
    "            # divide acc by seq_len\n",
    "            accuracy = accuracy/8\n",
    "            print(\"Epoch: \", t+1, \"Batch: \", batch+1, \"Accuracy: \", accuracy, \"Loss: \", tot_loss.item())\n",
    "\n",
    "    \n",
    "epochs = 10\n",
    "\n",
    "for t in range(1):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, Model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n",
      "done\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0] ---------------->\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m             \u001b[39mprint\u001b[39m(y_loc)\n\u001b[0;32m---> 35\u001b[0m val_loop(val_dataloader, Model, criterion)\n",
      "Cell \u001b[0;32mIn[50], line 23\u001b[0m, in \u001b[0;36mval_loop\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m     21\u001b[0m y_loc_tensor \u001b[39m=\u001b[39m y_loc_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39m#print(\"In shape: \", y_loc_tensor.shape)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m output \u001b[39m=\u001b[39m model(X, y_loc_tensor)\n\u001b[1;32m     24\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39m#print(\"Out shape\", output.shape)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 90\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     88\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x) \n\u001b[1;32m     89\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_outputs(y)\n\u001b[0;32m---> 90\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(y, x)\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, target, memory)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m# reshape target and memory\u001b[39;00m\n\u001b[1;32m     69\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m memory \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m)\n\u001b[1;32m     71\u001b[0m \u001b[39m#print(target.shape, memory.shape, target_mask.shape, \"------>\")\u001b[39;00m\n\u001b[1;32m     72\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_decoder(target, memory, tgt_mask \u001b[39m=\u001b[39m target_mask)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def val_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    acc_sum = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            y_loc = []\n",
    "            # append \n",
    "            # append 9 0s to y_loc\n",
    "            for i in range(9):\n",
    "                y_loc.append(0)\n",
    "            print(y_loc, \"---------------->\")\n",
    "            for i in range(8):\n",
    "                # convert y_loc to tensor of shape 1*seq_le\n",
    "                #print(y_loc)\n",
    "                y_loc_tensor = torch.tensor(y_loc)\n",
    "                # make it 2 dim\n",
    "                y_loc_tensor = y_loc_tensor.unsqueeze(0)\n",
    "                #print(\"In shape: \", y_loc_tensor.shape)\n",
    "                output = model(X, y_loc_tensor)\n",
    "                output = output.permute(1,0,2)\n",
    "                #print(\"Out shape\", output.shape)\n",
    "                output = output.argmax(2)\n",
    "                #print(output.shape, \"----->\")\n",
    "                output =output[:, i+1]\n",
    "                # append to y_loc\n",
    "                val = output.item()\n",
    "                y_loc[i+1] = val\n",
    "                #print(y_loc, \"------><<<<\")\n",
    "            print(\"done\")\n",
    "            y_loc = y_loc[1:]\n",
    "val_loop(val_dataloader, Model, criterion)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch, (X, y) in enumerate(dataloader):\n",
    "    #     # Compute the prediction and the loss\n",
    "    #     pred = model(X, y)\n",
    "    #     # reshape preds to 1 2 0\n",
    "    #     pred = pred.permute(1, 2,0)\n",
    "    #     #print(pred.shape, y.shape)\n",
    "    #     loss = loss_fn(pred, y)\n",
    "    #     # Backpropagation\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     # calculate accuracy\n",
    "    #     accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    #     # divide by batch size and number of chars\n",
    "    #     accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "    #     acc_tot += accuracy\n",
    "    #     tot_cnt += 1\n",
    "    #     acc_here = acc_tot/tot_cnt\n",
    "    #     # decode the preds using argmax\n",
    "    #     prediction = pred.argmax(1)\n",
    "    #     acc_here = acc_tot/tot_cnt\n",
    "    #     # Print the loss\n",
    "    #     if batch % 100 == 0:\n",
    "    #         loss, current = loss.item(), batch * len(X)\n",
    "    #         # print epoch, loss, batch, acc_here\n",
    "    #         print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] accuracy: {acc_here:>7f}\")\n",
    "\n",
    "# Create a Validation Loop  \n",
    "# def val_loop(dataloader, model, loss_fn):\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     model.eval()\n",
    "#     test_loss, correct = 0, 0\n",
    "#     acc_sum = 0\n",
    "#     cnt = 0\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             pred = model(X, y)\n",
    "#             pred = pred.permute(1, 2,0)\n",
    "#             test_loss += loss_fn(pred, y).item()\n",
    "#             accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "#             accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "#             acc_sum += accuracy\n",
    "#             cnt += 1\n",
    "#     test_loss /= num_batches\n",
    "#     acc_here = acc_sum/cnt\n",
    "#     print(\"Eval loop: loss: \", test_loss, \" accuracy: \", acc_here)\n",
    "            \n",
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(X, y)\n",
    "            pred = pred.permute(1, 2,0)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "            acc_sum += accuracy\n",
    "            cnt += 1\n",
    "    test_loss /= num_batches\n",
    "    acc_here = acc_sum/cnt\n",
    "    print(\"Eval loop: loss: \", test_loss, \" accuracy: \", acc_here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
