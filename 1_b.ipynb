{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torchvision.transforms import Lambda, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1, 15, 19,  5, 13,  5, 12, 22]]), tensor([[ 0, 15, 20, 24, 12, 16, 13, 24, 25]])]\n"
     ]
    }
   ],
   "source": [
    "# Create a Custom Dataset from train_data.csv and eval_data.csv\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pandas.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        # Remove the first row from the Dataframe\n",
    "        self.data = self.data.iloc[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() # Convert the tensor to a list\n",
    "        # Get the data from the Dataframe\n",
    "        data = self.data.iloc[idx]\n",
    "        # Convert the data to a numpy array\n",
    "        data = data[0] # First column of the Dataframe\n",
    "        data_num = []\n",
    "        for i in data:\n",
    "            data_num.append(ord(i)- 96)\n",
    "        data = np.array(data_num)\n",
    "        # Convert the data to a tensor\n",
    "        data = torch.from_numpy(data)\n",
    "        label = self.data.iloc[idx]\n",
    "        label = label[1]\n",
    "        label_num = []\n",
    "        for i in label:\n",
    "            label_num.append(ord(i)- 96)\n",
    "        # Append a 0 to the label_num list at the beginning\n",
    "        label_num.insert(0, 0)\n",
    "        label = np.array(label_num)\n",
    "        if self.transform:\n",
    "            data = self.transform(data) # Apply the transform on the data\n",
    "        return data, label\n",
    "\n",
    "# Write a DataLoader for the Custom Dataset\n",
    "train_dataset = CustomDataset('./A3 files/train_data.csv', transform=None)\n",
    "\n",
    "# Take only the first 200 examples\n",
    "# train_dataset.data = train_dataset.data.iloc[:200]\n",
    "\n",
    "# Split the Dataset into Train and Validation Datasets\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2)\n",
    "\n",
    "# Create a DataLoader for the Train and Validation Datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "############################# TESTING CODE #############################\n",
    "\n",
    "# Print out some data from the Train Dataset\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Injecting Some Information about the Relative or the Absolute Positioning of the tokens in the sequence\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len=8):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        position = []\n",
    "        pos_enc = torch.zeros(max_len, d_model) # Positional Encoding -> Max Length and the dimensions of the model\n",
    "        for i in range(max_len):\n",
    "            position.append(i)\n",
    "        position = torch.tensor(position).unsqueeze(1) # Got the pos -> value\n",
    "        # Now, you want to make the position term to be max_len, d_model\n",
    "        position_stacked = [position] * d_model \n",
    "        position = torch.cat(position_stacked, dim=1)\n",
    "        # Now to obtain the 10000^2i/d_model \n",
    "        div_term = torch.arange(0, d_model, 2) # The Value to be divided\n",
    "        div_term = div_term/d_model\n",
    "        div_term = div_term.type(torch.float64)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))   \n",
    "        pos_enc[:, 0::2] = torch.sin(position[:, 0::2]*div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position[:, 1::2]*div_term)\n",
    "        self.pos_enc = pos_enc\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc # This is placeholder -> Modify to include only the len given -> Not Max Len\n",
    "        #print(\"Positional Encoding Completed\")\n",
    "        return x\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosEncoding, self).__init__()\n",
    "        self.max_len = 1000\n",
    "        self.embedding_dim = 400\n",
    "        #self.embedding_layer = nn.Embedding(len(word2index), self.embedding_dim)\n",
    "    def forward(self, embedding):\n",
    "        # embeddings (b, s, d)\n",
    "        #embedding = self.embedding_layer(x)\n",
    "        pos_max = embedding.shape[1]\n",
    "        d_max = embedding.shape[2]\n",
    "        # create pos encoding matrix of size (b, s, d)\n",
    "        pos_encoding = torch.zeros((pos_max, d_max))\n",
    "        for pos in range(pos_max):\n",
    "            for i in range(0, d_max, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_max)))\n",
    "                pos_encoding[pos, i+1] = np.cos(pos / (10000 ** ((2 * (i+1))/d_max)))\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        pos_encoding = pos_encoding.repeat(embedding.shape[0], 1, 1)\n",
    "        #print(embedding.shape, pos_encoding.shape)\n",
    "        embedding = embedding + pos_encoding.to(embedding.device)\n",
    "        return embedding\n",
    "# Create a Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len = 8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PosEncoding()\n",
    "        self.transformer_decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        self.linear = nn.Linear(d_model, 27)\n",
    "        return\n",
    "    \n",
    "    def forward(self, target, memory):\n",
    "        #print(\"Decoder\")\n",
    "        seq_len = target.shape[1]\n",
    "        target = self.pos_enc(target)\n",
    "        # add batcg dim to target mask, batch dim = 1\n",
    "        target_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        # reshape target and memory\n",
    "        target = target.permute(1, 0, 2)\n",
    "        memory = memory.permute(1, 0, 2)\n",
    "        x = self.transformer_decoder(target, memory, tgt_mask = target_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(27, d_model)\n",
    "        self.embed_outputs = nn.Embedding(27, d_model)\n",
    "        self.encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        self.decoder = TransformerDecoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x, y, train=True):\n",
    "        if train == True:\n",
    "            x = self.embedding(x) # X = batch_size, seq_len, d_model\n",
    "            x = self.encoder(x) \n",
    "            y = self.embed_outputs(y)\n",
    "            y = self.decoder(y, x)\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            x = self.encoder(x)\n",
    "            # Now pass the output of the encoder to the decoder along with the start token\n",
    "            start_token = torch.tensor([0])\n",
    "            start_token = start_token.unsqueeze(0)\n",
    "            # print(\"Start Token Shape: \", start_token.shape)\n",
    "            for i in range(8):\n",
    "                input1 = self.embed_outputs(start_token)\n",
    "                y = self.decoder(input1, x)\n",
    "                y = torch.argmax(y, dim=2)\n",
    "                y = y[ -1, :]\n",
    "                y = y.unsqueeze(0)\n",
    "                start_token = torch.cat((start_token, y), dim=1)\n",
    "            y = start_token\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.374878  [    0/ 5599] accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.394493  [  100/ 5599] accuracy: 0.069307\n",
      "loss: 2.155067  [  200/ 5599] accuracy: 0.125000\n",
      "loss: 2.486464  [  300/ 5599] accuracy: 0.177741\n",
      "loss: 1.930989  [  400/ 5599] accuracy: 0.232544\n",
      "loss: 1.705398  [  500/ 5599] accuracy: 0.293413\n",
      "loss: 1.561754  [  600/ 5599] accuracy: 0.356281\n",
      "loss: 0.776073  [  700/ 5599] accuracy: 0.406740\n",
      "loss: 0.536523  [  800/ 5599] accuracy: 0.454276\n",
      "loss: 1.134993  [  900/ 5599] accuracy: 0.500139\n",
      "loss: 0.341447  [ 1000/ 5599] accuracy: 0.535964\n",
      "loss: 0.394169  [ 1100/ 5599] accuracy: 0.571185\n",
      "loss: 0.069830  [ 1200/ 5599] accuracy: 0.600541\n",
      "loss: 0.061981  [ 1300/ 5599] accuracy: 0.625000\n",
      "loss: 0.311970  [ 1400/ 5599] accuracy: 0.647662\n",
      "loss: 0.054717  [ 1500/ 5599] accuracy: 0.665806\n",
      "loss: 0.035295  [ 1600/ 5599] accuracy: 0.684338\n",
      "loss: 0.126707  [ 1700/ 5599] accuracy: 0.700176\n",
      "loss: 0.117031  [ 1800/ 5599] accuracy: 0.715019\n",
      "loss: 0.502866  [ 1900/ 5599] accuracy: 0.728367\n",
      "loss: 0.019121  [ 2000/ 5599] accuracy: 0.739568\n",
      "loss: 0.162798  [ 2100/ 5599] accuracy: 0.750059\n",
      "loss: 0.243918  [ 2200/ 5599] accuracy: 0.759882\n",
      "loss: 0.146550  [ 2300/ 5599] accuracy: 0.769013\n",
      "loss: 0.995005  [ 2400/ 5599] accuracy: 0.777384\n",
      "loss: 0.089288  [ 2500/ 5599] accuracy: 0.785136\n",
      "loss: 0.288979  [ 2600/ 5599] accuracy: 0.791955\n",
      "loss: 0.050737  [ 2700/ 5599] accuracy: 0.798501\n",
      "loss: 0.013825  [ 2800/ 5599] accuracy: 0.804757\n",
      "loss: 0.010634  [ 2900/ 5599] accuracy: 0.810712\n",
      "loss: 0.289439  [ 3000/ 5599] accuracy: 0.815811\n",
      "loss: 0.005461  [ 3100/ 5599] accuracy: 0.820743\n",
      "loss: 0.015545  [ 3200/ 5599] accuracy: 0.825250\n",
      "loss: 0.034386  [ 3300/ 5599] accuracy: 0.829143\n",
      "loss: 0.019902  [ 3400/ 5599] accuracy: 0.833137\n",
      "loss: 0.017289  [ 3500/ 5599] accuracy: 0.837368\n",
      "loss: 0.024804  [ 3600/ 5599] accuracy: 0.841225\n",
      "loss: 0.006233  [ 3700/ 5599] accuracy: 0.844839\n",
      "loss: 0.085594  [ 3800/ 5599] accuracy: 0.848165\n",
      "loss: 0.007497  [ 3900/ 5599] accuracy: 0.851512\n",
      "loss: 0.005813  [ 4000/ 5599] accuracy: 0.854536\n",
      "loss: 0.034673  [ 4100/ 5599] accuracy: 0.857718\n",
      "loss: 0.006248  [ 4200/ 5599] accuracy: 0.860837\n",
      "loss: 0.023337  [ 4300/ 5599] accuracy: 0.863840\n",
      "loss: 0.006377  [ 4400/ 5599] accuracy: 0.866309\n",
      "loss: 0.831739  [ 4500/ 5599] accuracy: 0.868668\n",
      "loss: 0.363105  [ 4600/ 5599] accuracy: 0.870789\n",
      "loss: 0.014201  [ 4700/ 5599] accuracy: 0.872979\n",
      "loss: 0.042413  [ 4800/ 5599] accuracy: 0.875000\n",
      "loss: 0.167515  [ 4900/ 5599] accuracy: 0.876913\n",
      "loss: 0.017952  [ 5000/ 5599] accuracy: 0.878974\n",
      "loss: 0.007832  [ 5100/ 5599] accuracy: 0.880710\n",
      "loss: 0.030713  [ 5200/ 5599] accuracy: 0.882426\n",
      "loss: 0.019249  [ 5300/ 5599] accuracy: 0.884173\n",
      "loss: 0.004940  [ 5400/ 5599] accuracy: 0.885947\n",
      "loss: 0.027125  [ 5500/ 5599] accuracy: 0.887793\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.002798  [    0/ 5599] accuracy: 1.000000\n",
      "loss: 0.004371  [  100/ 5599] accuracy: 0.985149\n",
      "loss: 0.008274  [  200/ 5599] accuracy: 0.986318\n",
      "loss: 0.016276  [  300/ 5599] accuracy: 0.985465\n",
      "loss: 0.046878  [  400/ 5599] accuracy: 0.983167\n",
      "loss: 0.004606  [  500/ 5599] accuracy: 0.984281\n",
      "loss: 0.004261  [  600/ 5599] accuracy: 0.985233\n",
      "loss: 0.002015  [  700/ 5599] accuracy: 0.986448\n",
      "loss: 0.010293  [  800/ 5599] accuracy: 0.985643\n",
      "loss: 0.006629  [  900/ 5599] accuracy: 0.985572\n",
      "loss: 0.001963  [ 1000/ 5599] accuracy: 0.986513\n",
      "loss: 0.007997  [ 1100/ 5599] accuracy: 0.987171\n",
      "loss: 0.001222  [ 1200/ 5599] accuracy: 0.987719\n",
      "loss: 0.295191  [ 1300/ 5599] accuracy: 0.986645\n",
      "loss: 0.050474  [ 1400/ 5599] accuracy: 0.984386\n",
      "loss: 0.120127  [ 1500/ 5599] accuracy: 0.984177\n",
      "loss: 0.003582  [ 1600/ 5599] accuracy: 0.983604\n",
      "loss: 0.002143  [ 1700/ 5599] accuracy: 0.983760\n",
      "loss: 0.673358  [ 1800/ 5599] accuracy: 0.984106\n",
      "loss: 0.001492  [ 1900/ 5599] accuracy: 0.984285\n",
      "loss: 0.035966  [ 2000/ 5599] accuracy: 0.984195\n",
      "loss: 0.061193  [ 2100/ 5599] accuracy: 0.983996\n",
      "loss: 0.016028  [ 2200/ 5599] accuracy: 0.983701\n",
      "loss: 0.002802  [ 2300/ 5599] accuracy: 0.983811\n",
      "loss: 0.002176  [ 2400/ 5599] accuracy: 0.983809\n",
      "loss: 0.166711  [ 2500/ 5599] accuracy: 0.983357\n",
      "loss: 0.018056  [ 2600/ 5599] accuracy: 0.983420\n",
      "loss: 0.003571  [ 2700/ 5599] accuracy: 0.983525\n",
      "loss: 0.004412  [ 2800/ 5599] accuracy: 0.983890\n",
      "loss: 0.005181  [ 2900/ 5599] accuracy: 0.984014\n",
      "loss: 0.005914  [ 3000/ 5599] accuracy: 0.984089\n",
      "loss: 0.024731  [ 3100/ 5599] accuracy: 0.984239\n",
      "loss: 0.002435  [ 3200/ 5599] accuracy: 0.984224\n",
      "loss: 0.001945  [ 3300/ 5599] accuracy: 0.984437\n",
      "loss: 0.001099  [ 3400/ 5599] accuracy: 0.984527\n",
      "loss: 0.005290  [ 3500/ 5599] accuracy: 0.984719\n",
      "loss: 0.492425  [ 3600/ 5599] accuracy: 0.984483\n",
      "loss: 0.003331  [ 3700/ 5599] accuracy: 0.984092\n",
      "loss: 0.507620  [ 3800/ 5599] accuracy: 0.983919\n",
      "loss: 0.017181  [ 3900/ 5599] accuracy: 0.983850\n",
      "loss: 0.001400  [ 4000/ 5599] accuracy: 0.984066\n",
      "loss: 0.026053  [ 4100/ 5599] accuracy: 0.984242\n",
      "loss: 0.001488  [ 4200/ 5599] accuracy: 0.984498\n",
      "loss: 0.001625  [ 4300/ 5599] accuracy: 0.984800\n",
      "loss: 0.008896  [ 4400/ 5599] accuracy: 0.985032\n",
      "loss: 0.001895  [ 4500/ 5599] accuracy: 0.984976\n",
      "loss: 0.026441  [ 4600/ 5599] accuracy: 0.984813\n",
      "loss: 0.003359  [ 4700/ 5599] accuracy: 0.984764\n",
      "loss: 0.004872  [ 4800/ 5599] accuracy: 0.984743\n",
      "loss: 0.001728  [ 4900/ 5599] accuracy: 0.984850\n",
      "loss: 0.004685  [ 5000/ 5599] accuracy: 0.984928\n",
      "loss: 0.004640  [ 5100/ 5599] accuracy: 0.984929\n",
      "loss: 0.016785  [ 5200/ 5599] accuracy: 0.984883\n",
      "loss: 0.001898  [ 5300/ 5599] accuracy: 0.985050\n",
      "loss: 0.001095  [ 5400/ 5599] accuracy: 0.985257\n",
      "loss: 0.000912  [ 5500/ 5599] accuracy: 0.985457\n"
     ]
    }
   ],
   "source": [
    "# Create a Transformer Model\n",
    "Model = Transformer(300, 2, 2, 300, 0.1, \"relu\")\n",
    "\n",
    "# Create a Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an Optimizer\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr=0.0001)\n",
    "\n",
    "# Create a Training Loop\n",
    "acc_tot = 0\n",
    "tot_cnt = 0\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    acc_tot = 0\n",
    "    tot_cnt = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute the prediction and the loss\n",
    "        pred = model(X, y, train=True)\n",
    "        # reshape preds to 1 2 0\n",
    "        pred = pred.permute(1,0,2)\n",
    "        pred = pred[:, :-1, :]\n",
    "        y = y[:, 1:]\n",
    "        #print(pred.shape, y.shape)\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        #print(pred.shape, y.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        # divide by batch size and number of chars\n",
    "        accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "        acc_tot += accuracy\n",
    "        tot_cnt += 1\n",
    "        acc_here = acc_tot/tot_cnt\n",
    "        # decode the preds using argmax\n",
    "        prediction = pred.argmax(1)\n",
    "        acc_here = acc_tot/tot_cnt\n",
    "        # print(\"Input\")\n",
    "        # print(X)\n",
    "        # print(\"Actual\")\n",
    "        # print(y)\n",
    "        # print(\"Label\")\n",
    "        # print(prediction)\n",
    "        # print(\"-------------------------------------------------------\")\n",
    "        # Print the loss\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print epoch, loss, batch, acc_here\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] accuracy: {acc_here:>7f}\")\n",
    "            \n",
    "# Train the Model\n",
    "epochs = 1\n",
    "\n",
    "for t in range(2):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, Model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loop: loss:  0.0  accuracy:  0.9980357142857142\n"
     ]
    }
   ],
   "source": [
    "# Create a Validation Loop  \n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    acc_sum = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X, y, train=True)\n",
    "            pred = pred.permute(1,0,2)\n",
    "            pred = pred[:, :-1, :]\n",
    "            y = y[:, 1:]\n",
    "            pred = pred.permute(0, 2, 1)\n",
    "            accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "            acc_sum += accuracy\n",
    "            cnt += 1\n",
    "    test_loss /= num_batches\n",
    "    acc_here = acc_sum/cnt\n",
    "    print(\"Eval loop: loss: \", test_loss, \" accuracy: \", acc_here)\n",
    "\n",
    "val_loop(val_dataloader, Model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loop: loss:  0.0  accuracy:  0.9979364682341171\n"
     ]
    }
   ],
   "source": [
    "# Run on the Eval Set as well\n",
    "eval_dataset = CustomDataset('./A3 files/eval_data.csv', transform=None)\n",
    "\n",
    "# Take only the first 200 examples\n",
    "# eval_dataset.data = eval_dataset.data.iloc[:200]\n",
    "\n",
    "# Create a DataLoader for the Eval Dataset\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_loop(eval_dataloader, Model, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
