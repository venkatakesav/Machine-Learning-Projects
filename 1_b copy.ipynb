{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torchvision.transforms import Lambda, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1,  3, 21, 14,  5, 15,  6,  2]]), tensor([[ 0, 17, 24,  9, 21, 25,  5, 18, 15]])]\n"
     ]
    }
   ],
   "source": [
    "# Create a Custom Dataset from train_data.csv and eval_data.csv\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pandas.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        # Remove the first row from the Dataframe\n",
    "        self.data = self.data.iloc[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() # Convert the tensor to a list\n",
    "        # Get the data from the Dataframe\n",
    "        data = self.data.iloc[idx]\n",
    "        # Convert the data to a numpy array\n",
    "        data = data[0] # First column of the Dataframe\n",
    "        data_num = []\n",
    "        for i in data:\n",
    "            data_num.append(ord(i)- 96)\n",
    "        data = np.array(data_num)\n",
    "        # Convert the data to a tensor\n",
    "        data = torch.from_numpy(data)\n",
    "        label = self.data.iloc[idx]\n",
    "        label = label[1]\n",
    "        label_num = []\n",
    "        for i in label:\n",
    "            label_num.append(ord(i)- 96)\n",
    "        # Append a 0 to the label_num list at the beginning\n",
    "        label_num.insert(0, 0)\n",
    "        label = np.array(label_num)\n",
    "        if self.transform:\n",
    "            data = self.transform(data) # Apply the transform on the data\n",
    "        return data, label\n",
    "\n",
    "# Write a DataLoader for the Custom Dataset\n",
    "train_dataset = CustomDataset('./A3 files/train_data.csv', transform=None)\n",
    "\n",
    "# Take only the first 200 examples\n",
    "train_dataset.data = train_dataset.data.iloc[:200]\n",
    "\n",
    "# Split the Dataset into Train and Validation Datasets\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2)\n",
    "\n",
    "# Create a DataLoader for the Train and Validation Datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "############################# TESTING CODE #############################\n",
    "\n",
    "# Print out some data from the Train Dataset\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Injecting Some Information about the Relative or the Absolute Positioning of the tokens in the sequence\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len=8):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        position = []\n",
    "        pos_enc = torch.zeros(max_len, d_model) # Positional Encoding -> Max Length and the dimensions of the model\n",
    "        for i in range(max_len):\n",
    "            position.append(i)\n",
    "        position = torch.tensor(position).unsqueeze(1) # Got the pos -> value\n",
    "        # Now, you want to make the position term to be max_len, d_model\n",
    "        position_stacked = [position] * d_model \n",
    "        position = torch.cat(position_stacked, dim=1)\n",
    "        # Now to obtain the 10000^2i/d_model \n",
    "        div_term = torch.arange(0, d_model, 2) # The Value to be divided\n",
    "        div_term = div_term/d_model\n",
    "        div_term = div_term.type(torch.float64)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))   \n",
    "        pos_enc[:, 0::2] = torch.sin(position[:, 0::2]*div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position[:, 1::2]*div_term)\n",
    "        self.pos_enc = pos_enc\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc # This is placeholder -> Modify to include only the len given -> Not Max Len\n",
    "        #print(\"Positional Encoding Completed\")\n",
    "        return x\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosEncoding, self).__init__()\n",
    "        self.max_len = 1000\n",
    "        self.embedding_dim = 400\n",
    "        #self.embedding_layer = nn.Embedding(len(word2index), self.embedding_dim)\n",
    "    def forward(self, embedding):\n",
    "        # embeddings (b, s, d)\n",
    "        # pos runs from 0, s-1\n",
    "        # i runs from 0, d-1\n",
    "        #embedding = self.embedding_layer(x)\n",
    "        pos_max = embedding.shape[1]\n",
    "        d_max = embedding.shape[2]\n",
    "        # create pos encoding matrix of size (b, s, d)\n",
    "        pos_encoding = torch.zeros((pos_max, d_max))\n",
    "        for pos in range(pos_max):\n",
    "            for i in range(0, d_max, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_max)))\n",
    "                pos_encoding[pos, i+1] = np.cos(pos / (10000 ** ((2 * (i+1))/d_max)))\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        pos_encoding = pos_encoding.repeat(embedding.shape[0], 1, 1)\n",
    "        #print(embedding.shape, pos_encoding.shape)\n",
    "        embedding = embedding + pos_encoding.to(embedding.device)\n",
    "        return embedding\n",
    "# Create a Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len = 8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PosEncoding()\n",
    "        self.transformer_decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        self.linear = nn.Linear(d_model, 27)\n",
    "        return\n",
    "    \n",
    "    def forward(self, target, memory):\n",
    "        #print(\"Decoder\")\n",
    "        seq_len = target.shape[1]\n",
    "        target = self.pos_enc(target)\n",
    "        # create target mask\n",
    "        # add batcg dim to target mask, batch dim = 1\n",
    "        target_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        # add batch dim to target mask of 1\n",
    "        #target_mask = target_mask.unsqueeze(0)\n",
    "        # reshape target and memory\n",
    "        target = target.permute(1, 0, 2)\n",
    "        memory = memory.permute(1, 0, 2)\n",
    "        x = self.transformer_decoder(target, memory, tgt_mask = target_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(27, d_model)\n",
    "        self.embed_outputs = nn.Embedding(27, d_model)\n",
    "        self.encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        self.decoder = TransformerDecoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x, tgt, train=True):\n",
    "        if train:\n",
    "            # Embed input sequence\n",
    "            x = self.embedding(x)  # x: batch_size, seq_len, d_model\n",
    "            x = self.encoder(x)  # Encoder output\n",
    "\n",
    "            # Prepare variables for autoregressive generation in training\n",
    "            outputs = []\n",
    "            next_input = tgt[:, 0].unsqueeze(1)  # Start with the first input token\n",
    "\n",
    "            # Loop through the sequence\n",
    "            for i in range(1, tgt.size(1)):\n",
    "                # Get embedding for the current input token\n",
    "                input_emb = self.embed_outputs(next_input)\n",
    "                output = self.decoder(input_emb, x)\n",
    "                # Predict the next output token\n",
    "                outputs.append(next_input)\n",
    "                # Use the actual next token from the target sequence as the next input\n",
    "                next_input = tgt[:, i].unsqueeze(1)\n",
    "            # Stack all the outputs\n",
    "            y = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            x = self.encoder(x)\n",
    "            # Start token is usually the first token in the target vocabulary\n",
    "            start_token = tgt[:, 0].unsqueeze(1)\n",
    "            # Initialize tensor for decoder outputs\n",
    "            outputs = start_token\n",
    "            # Autoregressively generate tokens using the decoder\n",
    "            for i in range(1, tgt.size(1)):\n",
    "                input_emb = self.embed_outputs(outputs)\n",
    "                output = self.decoder(input_emb, x)\n",
    "                next_token = torch.argmax(output[:, -1, :], dim=-1, keepdim=True)\n",
    "                outputs = torch.cat((outputs, next_token), dim=1)\n",
    "\n",
    "            y = outputs\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([1, 8]) torch.Size([1, 9])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     train_loop(train_dataloader, Model, criterion, optimizer)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(pred\u001b[39m.\u001b[39mshape, y\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     21\u001b[0m \u001b[39m# reshape preds to 1 2 0\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m,\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     23\u001b[0m \u001b[39m#print(pred.shape, y.shape)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "# Create a Transformer Model\n",
    "Model = Transformer(300, 2, 2, 300, 0.1, \"relu\")\n",
    "\n",
    "# Create a Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an Optimizer\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr=0.0001)\n",
    "\n",
    "# Create a Training Loop\n",
    "acc_tot = 0\n",
    "tot_cnt = 0\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    acc_tot = 0\n",
    "    tot_cnt = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute the prediction and the loss\n",
    "        pred = model(X, y, train=True)\n",
    "        print(pred.shape, y.shape)\n",
    "        # reshape preds to 1 2 0\n",
    "        pred = pred.permute(1, 2,0)\n",
    "        #print(pred.shape, y.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        # divide by batch size and number of chars\n",
    "        accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "        acc_tot += accuracy\n",
    "        tot_cnt += 1\n",
    "        acc_here = acc_tot/tot_cnt\n",
    "        # decode the preds using argmax\n",
    "        prediction = pred.argmax(1)\n",
    "        # print(\"Predcition: \")\n",
    "        # print(prediction)\n",
    "        # print(\"Label: \")\n",
    "        # print(y)\n",
    "        # print(\"Input: \")\n",
    "        # print(X)\n",
    "        # print(\"------------------------------------------------------\")\n",
    "        acc_here = acc_tot/tot_cnt\n",
    "        # Print the loss\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print epoch, loss, batch, acc_here\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] accuracy: {acc_here:>7f}\")\n",
    "            \n",
    "# Train the Model\n",
    "epochs = 10\n",
    "\n",
    "for t in range(1):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, Model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Validation Loop  \n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    acc_sum = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X,X,train=False)\n",
    "            # Remove the First Element\n",
    "            pred = pred[:, 1:]\n",
    "            print(\"Input\", X)\n",
    "            print(\"Prediction\", pred)\n",
    "            print(\"===========================================================\")\n",
    "            accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "            acc_sum += accuracy\n",
    "            cnt += 1\n",
    "            if cnt == 10:\n",
    "                break\n",
    "    test_loss /= num_batches\n",
    "    acc_here = acc_sum/cnt\n",
    "    print(\"Eval loop: loss: \", test_loss, \" accuracy: \", acc_here)\n",
    "\n",
    "Model = Transformer(300, 2, 2, 300, 0.1, \"relu\")\n",
    "\n",
    "val_loop(val_dataloader, Model, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
