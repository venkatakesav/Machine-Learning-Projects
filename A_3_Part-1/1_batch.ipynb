{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torchvision.transforms import Lambda, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 3, 26, 21, 21,  3, 18,  1, 22],\n",
      "        [ 2, 17, 26, 10, 22, 18, 15,  5],\n",
      "        [14, 18, 16, 22, 16,  4, 11, 14],\n",
      "        [13, 25,  2, 22, 21,  5, 19,  2],\n",
      "        [13, 10, 15,  6,  4,  8,  1, 15],\n",
      "        [18, 18, 14,  1,  1, 17, 24,  9],\n",
      "        [13,  4,  8, 14, 24, 16,  9, 25],\n",
      "        [ 2, 21, 11,  9, 18, 12, 15, 10],\n",
      "        [11, 14, 13, 26, 15, 14, 13,  9],\n",
      "        [13, 26, 22, 12,  5, 23,  8, 20],\n",
      "        [18,  8, 10, 26, 26, 16,  7, 12],\n",
      "        [ 7,  1,  3, 18,  4, 15, 23, 19],\n",
      "        [ 4, 23, 17, 15, 11, 23,  1,  1],\n",
      "        [18, 26, 16, 12, 19, 17, 11, 13],\n",
      "        [ 4, 23, 24, 21, 17, 13,  2,  2],\n",
      "        [15, 19, 10,  7,  5,  4, 23, 17],\n",
      "        [ 3, 14, 21, 10,  8, 23,  3, 21],\n",
      "        [15, 19, 11,  7,  3,  5, 23,  1],\n",
      "        [ 4, 20, 22,  1,  5, 11, 26, 10],\n",
      "        [ 2, 11,  1,  4, 21, 23, 14, 14],\n",
      "        [19, 13, 11, 19,  4, 26, 16,  4],\n",
      "        [17,  9, 22, 13, 17, 12, 24, 14],\n",
      "        [13,  4, 15, 17, 14, 25, 18, 24],\n",
      "        [13, 22, 23,  4, 16,  6,  4,  8],\n",
      "        [ 7, 13, 25, 18, 24,  9, 12, 15],\n",
      "        [19, 26, 25, 10, 17,  9, 21,  2],\n",
      "        [11, 10,  6, 16,  2, 15, 20, 25],\n",
      "        [24, 12, 26, 23,  5, 12, 19, 16],\n",
      "        [24, 22, 11, 15, 20, 19,  3,  8],\n",
      "        [ 1, 10,  9, 15,  7,  4,  9, 21],\n",
      "        [19, 22,  1, 16,  7, 26, 19, 19],\n",
      "        [22,  3, 26, 22, 11, 17,  4,  1]]), tensor([[ 0, 24, 23,  5,  4, 21,  9, 24, 24],\n",
      "        [ 0,  6, 14, 20,  7,  7,  8,  3, 17],\n",
      "        [ 0, 21,  1, 11, 13, 25, 16,  2,  6],\n",
      "        [ 0,  9, 19, 14,  4,  1, 10, 23,  2],\n",
      "        [ 0, 18, 19, 22, 22, 15,  2, 15, 11],\n",
      "        [ 0, 21, 21,  8, 23, 16, 17,  5,  6],\n",
      "        [ 0,  6, 18, 15,  7,  1,  8,  5,  7],\n",
      "        [ 0, 14, 23, 16, 13,  2, 10, 16,  4],\n",
      "        [ 0, 26,  3,  8, 19,  4,  1, 13,  5],\n",
      "        [ 0, 24, 20, 13, 14,  3,  9, 24,  1],\n",
      "        [ 0,  1, 13, 11, 23, 21, 16, 18,  8],\n",
      "        [ 0, 13, 24,  5,  2, 10,  7, 26, 13],\n",
      "        [ 0, 18, 21, 14, 22,  8, 18, 12, 11],\n",
      "        [ 0, 11, 25, 13,  6, 17,  8, 26, 19],\n",
      "        [ 0, 18, 22, 13, 11, 24, 17,  6, 17],\n",
      "        [ 0, 23, 21, 13,  7, 13, 13, 21, 13],\n",
      "        [ 0, 26, 23,  2, 14,  4, 17, 26, 23],\n",
      "        [ 0, 23, 10, 26, 22,  2, 14,  8, 13],\n",
      "        [ 0, 25,  5, 20,  1,  4, 19, 10,  5],\n",
      "        [ 0, 20,  3, 11, 11,  7, 13, 12, 11],\n",
      "        [ 0, 11,  8, 14, 11, 12, 19, 23, 23],\n",
      "        [ 0,  3,  5, 17, 25, 13, 22, 11, 19],\n",
      "        [ 0,  6, 19, 21,  3, 11, 18, 18,  9],\n",
      "        [ 0, 16,  9, 12, 16, 25, 15, 15,  3],\n",
      "        [ 0, 11, 16, 25, 18, 24, 14, 21, 12],\n",
      "        [ 0, 24, 23, 22, 17, 15, 16, 21, 14],\n",
      "        [ 0, 18,  2,  3,  3, 24,  2, 25,  8],\n",
      "        [ 0,  9,  3, 16, 25, 21,  5,  2,  2],\n",
      "        [ 0,  3, 12, 21, 17, 13, 11, 15, 10],\n",
      "        [ 0, 18, 13,  1, 18,  9, 23,  3,  7],\n",
      "        [ 0, 16,  1, 10,  9, 20, 17,  1, 15],\n",
      "        [ 0,  4,  4, 12, 24, 25, 11, 15, 16]])]\n"
     ]
    }
   ],
   "source": [
    "# Create a Custom Dataset from train_data.csv and eval_data.csv\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pandas.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        # Remove the first row from the Dataframe\n",
    "        self.data = self.data.iloc[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() # Convert the tensor to a list\n",
    "        # Get the data from the Dataframe\n",
    "        data = self.data.iloc[idx]\n",
    "        # Convert the data to a numpy array\n",
    "        data = data[0] # First column of the Dataframe\n",
    "        data_num = []\n",
    "        for i in data:\n",
    "            data_num.append(ord(i)- 96)\n",
    "        data = np.array(data_num)\n",
    "        # Convert the data to a tensor\n",
    "        data = torch.from_numpy(data)\n",
    "        label = self.data.iloc[idx]\n",
    "        label = label[1]\n",
    "        label_num = []\n",
    "        for i in label:\n",
    "            label_num.append(ord(i)- 96)\n",
    "        # Append a 0 to the label_num list at the beginning\n",
    "        label_num.insert(0, 0)\n",
    "        label = np.array(label_num)\n",
    "        if self.transform:\n",
    "            data = self.transform(data) # Apply the transform on the data\n",
    "        return data, label\n",
    "\n",
    "# Write a DataLoader for the Custom Dataset\n",
    "train_dataset = CustomDataset('./A3 files/train_data.csv', transform=None)\n",
    "\n",
    "# Take only the first 200 examples\n",
    "# train_dataset.data = train_dataset.data.iloc[:200]\n",
    "\n",
    "# Split the Dataset into Train and Validation Datasets\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2)\n",
    "\n",
    "# Create a DataLoader for the Train and Validation Datasets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "############################# TESTING CODE #############################\n",
    "\n",
    "# Print out some data from the Train Dataset\n",
    "for i, data in enumerate(train_dataloader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Injecting Some Information about the Relative or the Absolute Positioning of the tokens in the sequence\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len=8):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "        position = []\n",
    "        pos_enc = torch.zeros(max_len, d_model) # Positional Encoding -> Max Length and the dimensions of the model\n",
    "        for i in range(max_len):\n",
    "            position.append(i)\n",
    "        position = torch.tensor(position).unsqueeze(1) # Got the pos -> value\n",
    "        # Now, you want to make the position term to be max_len, d_model\n",
    "        position_stacked = [position] * d_model \n",
    "        position = torch.cat(position_stacked, dim=1)\n",
    "        # Now to obtain the 10000^2i/d_model \n",
    "        div_term = torch.arange(0, d_model, 2) # The Value to be divided\n",
    "        div_term = div_term/d_model\n",
    "        div_term = div_term.type(torch.float64)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))   \n",
    "        pos_enc[:, 0::2] = torch.sin(position[:, 0::2]*div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position[:, 1::2]*div_term)\n",
    "        self.pos_enc = pos_enc\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc # This is placeholder -> Modify to include only the len given -> Not Max Len\n",
    "        #print(\"Positional Encoding Completed\")\n",
    "        return x\n",
    "\n",
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PosEncoding, self).__init__()\n",
    "        self.max_len = 1000\n",
    "        self.embedding_dim = 400\n",
    "        #self.embedding_layer = nn.Embedding(len(word2index), self.embedding_dim)\n",
    "    def forward(self, embedding):\n",
    "        # embeddings (b, s, d)\n",
    "        #embedding = self.embedding_layer(x)\n",
    "        pos_max = embedding.shape[1]\n",
    "        d_max = embedding.shape[2]\n",
    "        # create pos encoding matrix of size (b, s, d)\n",
    "        pos_encoding = torch.zeros((pos_max, d_max))\n",
    "        for pos in range(pos_max):\n",
    "            for i in range(0, d_max, 2):\n",
    "                pos_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i)/d_max)))\n",
    "                pos_encoding[pos, i+1] = np.cos(pos / (10000 ** ((2 * (i+1))/d_max)))\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        pos_encoding = pos_encoding.repeat(embedding.shape[0], 1, 1)\n",
    "        #print(embedding.shape, pos_encoding.shape)\n",
    "        embedding = embedding + pos_encoding.to(embedding.device)\n",
    "        return embedding\n",
    "# Create a Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len = 8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Decoder\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.pos_enc = PosEncoding()\n",
    "        self.transformer_decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout, activation), num_layers)\n",
    "        self.linear = nn.Linear(d_model, 27)\n",
    "        return\n",
    "    \n",
    "    def forward(self, target, memory):\n",
    "        #print(\"Decoder\")\n",
    "        seq_len = target.shape[1]\n",
    "        target = self.pos_enc(target)\n",
    "        # add batcg dim to target mask, batch dim = 1\n",
    "        target_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        # reshape target and memory\n",
    "        target = target.permute(1, 0, 2)\n",
    "        memory = memory.permute(1, 0, 2)\n",
    "        x = self.transformer_decoder(target, memory, tgt_mask = target_mask)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Create a Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.1, activation=\"relu\"):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(27, d_model)\n",
    "        self.embed_outputs = nn.Embedding(27, d_model)\n",
    "        self.encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        self.decoder = TransformerDecoder(d_model, nhead, num_layers, dim_feedforward, dropout, activation)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x, y, train=True):\n",
    "        if train == True:\n",
    "            x = self.embedding(x) # X = batch_size, seq_len, d_model\n",
    "            x = self.encoder(x) \n",
    "            y = self.embed_outputs(y)\n",
    "            y = self.decoder(y, x)\n",
    "        else:\n",
    "            x = self.embedding(x)\n",
    "            x = self.encoder(x)\n",
    "            # Now pass the output of the encoder to the decoder along with the start token\n",
    "            start_token = torch.tensor([0])\n",
    "            start_token = start_token.unsqueeze(0)\n",
    "            # print(\"Start Token Shape: \", start_token.shape)\n",
    "            for i in range(8):\n",
    "                input1 = self.embed_outputs(start_token)\n",
    "                y = self.decoder(input1, x)\n",
    "                y = torch.argmax(y, dim=2)\n",
    "                y = y[-1, :]\n",
    "                y = y.unsqueeze(0)\n",
    "                start_token = torch.cat((start_token, y), dim=1)\n",
    "            y = start_token\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.458588  [    0/ 5599] accuracy: 0.046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.769725  [ 3200/ 5599] accuracy: 0.253481\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.529419  [    0/ 5599] accuracy: 0.847656\n",
      "loss: 0.107687  [ 3200/ 5599] accuracy: 0.962446\n"
     ]
    }
   ],
   "source": [
    "# Create a Transformer Model\n",
    "Model = Transformer(300, 2, 2, 300, 0.1, \"relu\")\n",
    "\n",
    "# Create a Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an Optimizer\n",
    "optimizer = torch.optim.Adam(Model.parameters(), lr=0.0001)\n",
    "\n",
    "# Create a Training Loop\n",
    "acc_tot = 0\n",
    "tot_cnt = 0\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    acc_tot = 0\n",
    "    tot_cnt = 0\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute the prediction and the loss\n",
    "        pred = model(X, y, train=True)\n",
    "        # reshape preds to 1 2 0\n",
    "        pred = pred.permute(1,0,2)\n",
    "        pred = pred[:, :-1, :]\n",
    "        y = y[:, 1:]\n",
    "        #print(pred.shape, y.shape)\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        #print(pred.shape, y.shape)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        # divide by batch size and number of chars\n",
    "        accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "        acc_tot += accuracy\n",
    "        tot_cnt += 1\n",
    "        acc_here = acc_tot/tot_cnt\n",
    "        # decode the preds using argmax\n",
    "        prediction = pred.argmax(1)\n",
    "        acc_here = acc_tot/tot_cnt\n",
    "        # print(\"Input\")\n",
    "        # print(X)\n",
    "        # print(\"Actual\")\n",
    "        # print(y)\n",
    "        # print(\"Label\")\n",
    "        # print(prediction)\n",
    "        # print(\"-------------------------------------------------------\")\n",
    "        # Print the loss\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # print epoch, loss, batch, acc_here\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}] accuracy: {acc_here:>7f}\")\n",
    "            \n",
    "# Train the Model\n",
    "epochs = 10\n",
    "\n",
    "for t in range(2):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, Model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loop: loss:  0.0  accuracy:  0.9999112215909091\n"
     ]
    }
   ],
   "source": [
    "# Create a Validation Loop  \n",
    "def val_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    acc_sum = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X, y, train=True)\n",
    "            pred = pred.permute(1,0,2)\n",
    "            pred = pred[:, :-1, :]\n",
    "            y = y[:, 1:]\n",
    "            pred = pred.permute(0, 2, 1)\n",
    "            accuracy = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            accuracy = accuracy/(y.shape[0]*y.shape[1])\n",
    "            acc_sum += accuracy\n",
    "            cnt += 1\n",
    "    test_loss /= num_batches\n",
    "    acc_here = acc_sum/cnt\n",
    "    print(\"Eval loop: loss: \", test_loss, \" accuracy: \", acc_here)\n",
    "\n",
    "val_loop(val_dataloader, Model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval loop: loss:  0.0  accuracy:  0.9990620310155077\n"
     ]
    }
   ],
   "source": [
    "# Run on the Eval Set as well\n",
    "eval_dataset = CustomDataset('./A3 files/eval_data.csv', transform=None)\n",
    "\n",
    "# Take only the first 200 examples\n",
    "# eval_dataset.data = eval_dataset.data.iloc[:200]\n",
    "\n",
    "# Create a DataLoader for the Eval Dataset\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_loop(eval_dataloader, Model, criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
